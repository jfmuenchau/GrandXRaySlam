{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:24:58.800533Z",
     "iopub.status.busy": "2025-09-18T17:24:58.800316Z",
     "iopub.status.idle": "2025-09-18T17:25:14.893058Z",
     "shell.execute_reply": "2025-09-18T17:25:14.892465Z",
     "shell.execute_reply.started": "2025-09-18T17:24:58.800515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchmetrics.functional import accuracy, recall, precision, auroc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import accuracy, recall, precision, auroc\n",
    "from PIL import Image, ImageEnhance, ImageOps, ImageFilter\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE_DIR = \"/kaggle/input/grand-xray-slam-division-a\"\n",
    "label_columns = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n",
    "    'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "    'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n",
    "]\n",
    "\n",
    "\n",
    "def setup_seed(seed=None):\n",
    "    if seed is None:\n",
    "        return\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation Spaces and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.894797Z",
     "iopub.status.busy": "2025-09-18T17:25:14.894306Z",
     "iopub.status.idle": "2025-09-18T17:25:14.902905Z",
     "shell.execute_reply": "2025-09-18T17:25:14.902200Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.894777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# used to freeze layers in a model\n",
    "def freeze_all(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "def get_resnet18(num_classes=14, fine_tune=True):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if fine_tune:\n",
    "        freeze_all(model)\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.fc = nn.Linear(512, num_classes)\n",
    "    return model\n",
    "\n",
    "def get_resnet34(num_classes=14, fine_tune=True):\n",
    "    model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if fine_tune:\n",
    "        freeze_all(model)\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.fc = nn.Linear(512, num_classes)\n",
    "    return model\n",
    "\n",
    "def get_effnetb0(num_classes=14, fine_tune=True):\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if fine_tune:\n",
    "        freeze_all(model)\n",
    "        for idx in range(6, 8):\n",
    "            for param in model.features[idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_convnext_tiny(num_classes=14, fine_tune=True):\n",
    "    model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if fine_tune:\n",
    "        freeze_all(model)\n",
    "        for param in model.features[6].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.classifier[2] = nn.Linear(in_features=768, out_features=num_classes)\n",
    "    return model\n",
    "\n",
    "# returns getter function and number of in features of the classifier\n",
    "models_ = {\n",
    "    \"res18\": (get_resnet18, 512),\n",
    "    \"res34\": (get_resnet34, 512),\n",
    "    \"effb0\": (get_effnetb0, 1280),\n",
    "    \"convnext\" : (get_convnext_tiny, 768),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.903764Z",
     "iopub.status.busy": "2025-09-18T17:25:14.903553Z",
     "iopub.status.idle": "2025-09-18T17:25:14.925991Z",
     "shell.execute_reply": "2025-09-18T17:25:14.925376Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.903749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up transforms\n",
    "basic_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # resize to 224x224\n",
    "    transforms.ToTensor(), # convert to tensor [0,1]\n",
    "    transforms.Normalize( # normalize with ImageNet stats\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# custom transforms with adaptive magnitude\n",
    "def shear_x(img, magnitude):\n",
    "    level = magnitude * 0.3 * random.choice([-1, 1])\n",
    "    return img.transform(img.size, Image.AFFINE, (1, level, 0, 0, 1, 0))\n",
    "\n",
    "def shear_y(img, magnitude):\n",
    "    level = magnitude * 0.3 * random.choice([-1, 1])\n",
    "    return img.transform(img.size, Image.AFFINE, (1, 0, 0, level, 1, 0))\n",
    "\n",
    "def translate_x(img, magnitude):\n",
    "    max_shift = 0.3 * img.size[0]\n",
    "    level = magnitude * max_shift * random.choice([-1, 1])\n",
    "    return img.transform(img.size, Image.AFFINE, (1, 0, level, 0, 1, 0))\n",
    "\n",
    "def translate_y(img, magnitude):\n",
    "    max_shift = 0.3 * img.size[1]\n",
    "    level = magnitude * max_shift * random.choice([-1, 1])\n",
    "    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, level))\n",
    "\n",
    "def rotate(img, magnitude):\n",
    "    degrees = magnitude * 30 * random.choice([-1, 1])\n",
    "    return img.rotate(degrees)\n",
    "\n",
    "def contrast(img, magnitude):\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    factor = 1.0 + (magnitude * random.choice([-0.9, 0.9]))\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def brightness(img, magnitude):\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    factor = 1.0 + (magnitude * random.choice([-0.9, 0.9]))\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def sharpness(img, magnitude):\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    factor = 1.0 + (magnitude * random.choice([-0.9, 0.9]))\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def equalize(img, magnitude=None):\n",
    "    return ImageOps.equalize(img)\n",
    "\n",
    "def gaussian_blur(img, magnitude):\n",
    "    radius = magnitude * 2\n",
    "    return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "\n",
    "def identity(img, magnitude=None):\n",
    "    return img\n",
    "\n",
    "augmentation_space = [\n",
    "    shear_x, shear_y, \n",
    "    translate_x, translate_y,\n",
    "    rotate, equalize, \n",
    "    contrast, brightness, \n",
    "    sharpness, identity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.926761Z",
     "iopub.status.busy": "2025-09-18T17:25:14.926520Z",
     "iopub.status.idle": "2025-09-18T17:25:14.943121Z",
     "shell.execute_reply": "2025-09-18T17:25:14.942518Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.926735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AdaAugment can be used to control augmentation magnitudes and operations\n",
    "\n",
    "class AdaAugment:\n",
    "\n",
    "    def __init__(self, rand_m, rand_t):\n",
    "        self.key_transform = {}\n",
    "        self.key_magnitude = {}\n",
    "        self.transforms = augmentation_space\n",
    "        self.rand_m = rand_m\n",
    "        self.rand_t = rand_t\n",
    "\n",
    "    def set(self, keys, m, transform_idx=None):\n",
    "        for i, key in enumerate(keys):\n",
    "            self.key_magnitude[key] = m[i].cpu().detach()\n",
    "        \n",
    "        if transform_idx is not None:\n",
    "            for i, key in enumerate(keys):\n",
    "                self.key_transform[key] = transform_idx[i].cpu().detach()\n",
    "\n",
    "    def __call__(self, key, img):\n",
    "        # Get Magnitude for the sample\n",
    "        m = self.key_magnitude.get(key)\n",
    "        if m is None:  \n",
    "            if self.rand_m:\n",
    "                if random.random() < 0.4:\n",
    "                    return img  # skip transform\n",
    "                m = random.random() # select a random magnitude\n",
    "            else:\n",
    "                m = 0\n",
    "        else:\n",
    "            m = float(m)\n",
    "    \n",
    "        # Get transform\n",
    "        t = self.key_transform.get(key)\n",
    "        if t is None:\n",
    "            t = random.choice(self.transforms) if self.rand_t else self.transforms[-1] # if rand_t => select random augementation => else => use identiy as first transformation\n",
    "        else:\n",
    "            t = self.transforms[int(t)]\n",
    "\n",
    "        return t(img, m) # return applied transformation with corresponding magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.944947Z",
     "iopub.status.busy": "2025-09-18T17:25:14.944644Z",
     "iopub.status.idle": "2025-09-18T17:25:14.956696Z",
     "shell.execute_reply": "2025-09-18T17:25:14.956026Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.944923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Focal Loss for fine tuninng\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weights:List=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weights = weights\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            logit, target,\n",
    "            reduction=\"none\"\n",
    "        )\n",
    "        probs = torch.exp(-bce_loss)\n",
    "        F_loss = self.weights.to(target.device) * (1-probs) ** self.gamma * bce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == \"none\":\n",
    "            return F_loss   \n",
    "        else:\n",
    "            return F_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.957615Z",
     "iopub.status.busy": "2025-09-18T17:25:14.957350Z",
     "iopub.status.idle": "2025-09-18T17:25:14.974353Z",
     "shell.execute_reply": "2025-09-18T17:25:14.973630Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.957592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Saves per sample information such as previous loss etc.\n",
    "\n",
    "class ValueMemory:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Stores the last value per key (no EMA).\n",
    "        \"\"\"\n",
    "        self.values = {}\n",
    "\n",
    "    def __call__(self, keys, vals):\n",
    "        \"\"\"\n",
    "        keys: list of sample identifiers\n",
    "        vals: torch.Tensor of shape (len(keys), D)\n",
    "        Returns: current stored values, previous stored values\n",
    "        \"\"\"\n",
    "        stored_list = []\n",
    "        new_list = []\n",
    "\n",
    "        for i, key in enumerate(keys):\n",
    "            val = vals[i]\n",
    "            if key not in self.values:\n",
    "                old = val.clone()  # nothing stored yet → use current\n",
    "            else:\n",
    "                old = self.values[key]\n",
    "\n",
    "            self.values[key] = val  # overwrite with last value\n",
    "\n",
    "            stored_list.append(old.unsqueeze(0))\n",
    "            new_list.append(self.values[key].unsqueeze(0))\n",
    "\n",
    "        stored = torch.cat(stored_list, dim=0)  # previous values\n",
    "        return stored\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"\n",
    "        Access the stored value for a single key\n",
    "        \"\"\"\n",
    "        return self.values.get(key, None)\n",
    "\n",
    "    def get_multi(self, keys):\n",
    "        \"\"\"\n",
    "        Access stored values for multiple keys\n",
    "        \"\"\"\n",
    "        return torch.stack([self.values[k] for k in keys])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the AdaAugment Agent\n",
    "+ Critic => returns value for a state\n",
    "+ Actor => Parameterizes a beta distribution that is used to sample augmentation magnitudes\n",
    "+ Controller => Parameterizes a categorical distribution that is used to sample transformations from the augmentation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.975328Z",
     "iopub.status.busy": "2025-09-18T17:25:14.975132Z",
     "iopub.status.idle": "2025-09-18T17:25:14.992007Z",
     "shell.execute_reply": "2025-09-18T17:25:14.991429Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.975308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_features, hidden, out_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, hidden)\n",
    "        self.alpha_head = nn.Linear(hidden, out_features)\n",
    "        self.beta_head = nn.Linear(hidden, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(self.layer_norm1(x)))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        return torch.softmax(self.alpha_head(x), dim=-1) + 1, torch.softmax(self.beta_head(x), dim=-1) + 1\n",
    "\n",
    "    def get_dist(self, x):\n",
    "        alpha, beta = self(x)\n",
    "        dist = torch.distributions.Beta(alpha, beta)\n",
    "        return dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, hidden)\n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(self.layer_norm1(x)))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class Controller(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden)\n",
    "        self.linear2 = nn.Linear(hidden, hidden)\n",
    "        self.head = nn.Linear(hidden, len(augmentation_space))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(self.layer_norm1(x)))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        return self.head(x)\n",
    "    \n",
    "    def get_dist(self, x):\n",
    "        out = self(x)\n",
    "        dist = torch.distributions.Categorical(logits=out)\n",
    "        return dist\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, control=False, actor=False):\n",
    "        super().__init__()\n",
    "        self.val_memory = ValueMemory()\n",
    "        self.loss_memory = ValueMemory()\n",
    "\n",
    "        self.critic = Critic(in_features=in_features, hidden=128)\n",
    "\n",
    "        self.store_ = {}\n",
    "\n",
    "        self.control_ = control\n",
    "        self.actor_ = actor\n",
    "        if control:\n",
    "            self.controller = Controller(in_features=in_features, hidden=128)\n",
    "\n",
    "        if actor:\n",
    "            self.actor = Actor(in_features=in_features, hidden=128, out_features=1) \n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(\n",
    "            params=self.actor.parameters(), lr=3e-5, weight_decay=5e-4\n",
    "        )\n",
    "        self.critic_optimizer = torch.optim.Adam(\n",
    "            params=self.critic.parameters(), lr=3e-5, weight_decay=5e-4\n",
    "        )\n",
    "\n",
    "    def action(self, state):\n",
    "        action_actor = None\n",
    "        action_controller = None\n",
    "        if self.actor_:  \n",
    "            dist = self.actor.get_dist(state.detach())\n",
    "            action_actor = dist.sample()\n",
    "            self.store_[\"action_actor\"] = action_actor\n",
    "            self.store_[\"dist_actor\"] = dist\n",
    "        if self.control_:\n",
    "            dist = self.controller.get_dist(state.detach())\n",
    "            action_controller = dist.sample()\n",
    "            self.store_[\"action_controller\"] = action_controller\n",
    "            self.store_[\"dist_controller\"] = dist\n",
    "\n",
    "        return action_actor, action_controller\n",
    "\n",
    "    def update(self, key, state, reward):\n",
    "        value = self.critic(state)\n",
    "        prev_state = self.val_memory(key, state)\n",
    "        prev_value = self.critic(prev_state)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            td_target = reward + 0.99 * value\n",
    "\n",
    "        if self.actor_ and self.control_:\n",
    "            dist_actor, action_actor = self.store_[\"dist_actor\"], self.store_[\"action_actor\"]\n",
    "            dist_control, action_control = self.store_[\"dist_controller\"], self.store_[\"action_controller\"]\n",
    "\n",
    "            log_prob_actor = dist_actor.log_prob(action_actor)\n",
    "            log_prob_control = dist_control.log_prob(action_control)\n",
    "\n",
    "            log_prob = log_prob_actor + log_prob_control  \n",
    "\n",
    "        elif self.actor_:\n",
    "            dist, action = self.store_[\"dist_actor\"], self.store_[\"action_actor\"]\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        elif self.control_:\n",
    "            dist, action = self.store_[\"dist_controller\"], self.store_[\"action_controller\"]\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        actor_loss = -(log_prob * (td_target - prev_value.detach())).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        critic_loss = torch.nn.functional.mse_loss(td_target, prev_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "+ Class weights\n",
    "+ Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:14.992889Z",
     "iopub.status.busy": "2025-09-18T17:25:14.992631Z",
     "iopub.status.idle": "2025-09-18T17:25:15.010802Z",
     "shell.execute_reply": "2025-09-18T17:25:15.010026Z",
     "shell.execute_reply.started": "2025-09-18T17:25:14.992864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, train=True, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.label_columns = [\n",
    "            'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n",
    "            'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "            'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"img_path\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.df.iloc[idx][self.label_columns].values.astype(np.float32)\n",
    "        key = self.df.iloc[idx][\"Image_name\"]\n",
    "        \n",
    "        if self.train and self.transform is not None:\n",
    "            img = self.transform(key, img)\n",
    "\n",
    "        img = basic_transforms(img)\n",
    "\n",
    "        if not self.train:\n",
    "            return key, img\n",
    "\n",
    "        return key, img, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:15.011587Z",
     "iopub.status.busy": "2025-09-18T17:25:15.011418Z",
     "iopub.status.idle": "2025-09-18T17:25:15.024850Z",
     "shell.execute_reply": "2025-09-18T17:25:15.024179Z",
     "shell.execute_reply.started": "2025-09-18T17:25:15.011573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_class_weights(df):\n",
    "    weights = []\n",
    "    label_columns = [\n",
    "        'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n",
    "        'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n",
    "        'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n",
    "    ]\n",
    "\n",
    "    for label in label_columns:\n",
    "        percent = df[label].sum() / len(df)\n",
    "        weights.append(percent)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt entire Model to new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:25:38.230507Z",
     "iopub.status.busy": "2025-09-18T17:25:38.230018Z",
     "iopub.status.idle": "2025-09-18T17:25:38.736947Z",
     "shell.execute_reply": "2025-09-18T17:25:38.736313Z",
     "shell.execute_reply.started": "2025-09-18T17:25:38.230478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atelectasis: 0.362\n",
      "\n",
      "Cardiomegaly: 0.325\n",
      "\n",
      "Consolidation: 0.273\n",
      "\n",
      "Edema: 0.248\n",
      "\n",
      "Enlarged Cardiomediastinum: 0.352\n",
      "\n",
      "Fracture: 0.138\n",
      "\n",
      "Lung Lesion: 0.110\n",
      "\n",
      "Lung Opacity: 0.452\n",
      "\n",
      "No Finding: 0.316\n",
      "\n",
      "Pleural Effusion: 0.319\n",
      "\n",
      "Pleural Other: 0.066\n",
      "\n",
      "Pneumonia: 0.132\n",
      "\n",
      "Pneumothorax: 0.082\n",
      "\n",
      "Support Devices: 0.351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"/kaggle/input/grand-xray-slam-division-a/train1.csv\")\n",
    "train_csv[\"img_path\"] = train_csv[\"Image_name\"].apply(lambda x: os.path.join(BASE_DIR, \"train1\", x))\n",
    "submission_df = pd.read_csv(\"/kaggle/input/grand-xray-slam-division-a/sample_submission_1.csv\")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_csv,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_csv[\"No Finding\"]\n",
    ")\n",
    "\n",
    "submission_df[\"img_path\"] = submission_df[\"Image_name\"].apply(lambda x : os.path.join(BASE_DIR, \"test1\", x))\n",
    "class_weights = get_class_weights(train_df)\n",
    "zipped_class_weights = list(zip(label_columns, class_weights))\n",
    "\n",
    "for class_, weight in zipped_class_weights:\n",
    "    print(f\"{class_}: {weight:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:35:14.625864Z",
     "iopub.status.busy": "2025-09-18T17:35:14.625251Z",
     "iopub.status.idle": "2025-09-18T17:35:14.630983Z",
     "shell.execute_reply": "2025-09-18T17:35:14.630409Z",
     "shell.execute_reply.started": "2025-09-18T17:35:14.625838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "ada_augment=AdaAugment(rand_m=True, rand_t=False)\n",
    "train_set = XRayDataset(train_df, train=True)\n",
    "val_set = XRayDataset(val_df, train=True)\n",
    "test_set = XRayDataset(submission_df, train=False)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:34:03.052551Z",
     "iopub.status.busy": "2025-09-18T17:34:03.052059Z",
     "iopub.status.idle": "2025-09-18T17:34:03.192381Z",
     "shell.execute_reply": "2025-09-18T17:34:03.191602Z",
     "shell.execute_reply.started": "2025-09-18T17:34:03.052527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Models and functions\n",
    "\n",
    "model = get_effnetb0(fine_tune=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(class_weights).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T17:35:58.699652Z",
     "iopub.status.busy": "2025-09-18T17:35:58.699079Z",
     "iopub.status.idle": "2025-09-18T17:42:05.772903Z",
     "shell.execute_reply": "2025-09-18T17:42:05.771840Z",
     "shell.execute_reply.started": "2025-09-18T17:35:58.699631Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 489/2685 [06:06<27:28,  1.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1906783998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for keys, imgs, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "        imgs, targets = imgs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = bce_loss(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for keys, imgs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "            imgs, targets = imgs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = bce_loss(logits, targets)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    all_probs = torch.cat(all_probs, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    val_accuracy  = accuracy(all_probs, all_targets.int(), task=\"multilabel\", num_labels=14)\n",
    "    val_precision = precision(all_probs, all_targets.int(), task=\"multilabel\", num_labels=14)\n",
    "    val_recall    = recall(all_probs, all_targets.int(), task=\"multilabel\", num_labels=14)\n",
    "    val_auroc     = auroc(all_probs, all_targets.int(), task=\"multilabel\", num_labels=14)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Acc: {val_accuracy:.4f} | \"\n",
    "        f\"Prec: {val_precision:.4f} | \"\n",
    "        f\"Rec: {val_recall:.4f} | \"\n",
    "        f\"AUROC: {val_auroc:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning using AdaAugment and Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13449579,
     "sourceId": 112899,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
