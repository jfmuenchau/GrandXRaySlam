{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112899,"databundleVersionId":13449579,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torchmetrics.functional import accuracy, recall, precision, auroc\nfrom torch.utils.data import Dataset, DataLoader\nimport timm\nimport numpy as np\nimport pandas as pd\nimport random\nfrom typing import List\nfrom tqdm import tqdm\nfrom torchmetrics.functional import accuracy, recall, precision, auroc\nfrom PIL import Image, ImageEnhance, ImageOps, ImageFilter\nimport os\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\n\nBASE_DIR = \"/kaggle/input/grand-xray-slam-division-a\"\nlabel_columns = [\n    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n    'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n    'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n]\n\n\ndef setup_seed(seed=None):\n    if seed is None:\n        return\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:33.758598Z","iopub.execute_input":"2025-09-21T09:53:33.758806Z","iopub.status.idle":"2025-09-21T09:53:51.858428Z","shell.execute_reply.started":"2025-09-21T09:53:33.758782Z","shell.execute_reply":"2025-09-21T09:53:51.857680Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Augmentation Spaces and Utilities","metadata":{}},{"cell_type":"code","source":"# used to freeze layers in a model\ndef freeze_all(model):\n    for param in model.parameters():\n        param.requires_grad=False\n\ndef get_resnet18(num_classes=14, fine_tune=True):\n    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\n    if fine_tune:\n        freeze_all(model)\n        for param in model.layer4.parameters():\n            param.requires_grad = True\n\n    model.fc = nn.Linear(512, num_classes)\n    return model\n\ndef get_resnet34(num_classes=14, fine_tune=True):\n    model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n\n    if fine_tune:\n        freeze_all(model)\n        for param in model.layer4.parameters():\n            param.requires_grad = True\n\n    model.fc = nn.Linear(512, num_classes)\n    return model\n\ndef get_effnetb0(num_classes=14, fine_tune=True):\n    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n\n    if fine_tune:\n        freeze_all(model)\n        for idx in range(6, 8):\n            for param in model.features[idx].parameters():\n                param.requires_grad = True\n\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2, inplace=True),\n        nn.Linear(in_features=1280, out_features=num_classes)\n    )\n    \n    return model\n\ndef get_convnext_tiny(num_classes=14, fine_tune=True):\n    model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n\n    if fine_tune:\n        freeze_all(model)\n        for param in model.features[6].parameters():\n            param.requires_grad = True\n\n    model.classifier[2] = nn.Linear(in_features=768, out_features=num_classes)\n    return model\n\n# returns getter function and number of in features of the classifier\nmodels_ = {\n    \"res18\": (get_resnet18, 512),\n    \"res34\": (get_resnet34, 512),\n    \"effb0\": (get_effnetb0, 1280),\n    \"convnext\" : (get_convnext_tiny, 768),\n}","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:51.860036Z","iopub.execute_input":"2025-09-21T09:53:51.860470Z","iopub.status.idle":"2025-09-21T09:53:51.869727Z","shell.execute_reply.started":"2025-09-21T09:53:51.860451Z","shell.execute_reply":"2025-09-21T09:53:51.869012Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set up transforms\nbasic_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # resize to 224x224\n    transforms.ToTensor(), # convert to tensor [0,1]\n    transforms.Normalize( # normalize with ImageNet stats\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# custom transforms with adaptive magnitude\ndef shear_x(img, magnitude):\n    level = magnitude * 0.3 * random.choice([-1, 1])\n    return img.transform(img.size, Image.AFFINE, (1, level, 0, 0, 1, 0))\n\ndef shear_y(img, magnitude):\n    level = magnitude * 0.3 * random.choice([-1, 1])\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, level, 1, 0))\n\ndef translate_x(img, magnitude):\n    max_shift = 0.3 * img.size[0]\n    level = magnitude * max_shift * random.choice([-1, 1])\n    return img.transform(img.size, Image.AFFINE, (1, 0, level, 0, 1, 0))\n\ndef translate_y(img, magnitude):\n    max_shift = 0.3 * img.size[1]\n    level = magnitude * max_shift * random.choice([-1, 1])\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, level))\n\ndef rotate(img, magnitude):\n    degrees = magnitude * 30 * random.choice([-1, 1])\n    return img.rotate(degrees)\n\ndef contrast(img, magnitude):\n    enhancer = ImageEnhance.Contrast(img)\n    factor = 1.0 + (magnitude * random.choice([-0.9, 0.9]))\n    return enhancer.enhance(factor)\n\ndef brightness(img, magnitude):\n    enhancer = ImageEnhance.Brightness(img)\n    factor = 1.0 + (magnitude * random.choice([-0.9, 0.9]))\n    return enhancer.enhance(factor)\n\ndef sharpness(img, magnitude):\n    enhancer = ImageEnhance.Sharpness(img)\n    factor = 1.0 + (magnitude * random.choice([-0.9, 0.9]))\n    return enhancer.enhance(factor)\n\ndef equalize(img, magnitude=None):\n    return ImageOps.equalize(img)\n\ndef gaussian_blur(img, magnitude):\n    radius = magnitude * 2\n    return img.filter(ImageFilter.GaussianBlur(radius))\n\ndef identity(img, magnitude=None):\n    return img\n\naugmentation_space = [\n    shear_x, shear_y, \n    translate_x, translate_y,\n    rotate, equalize, \n    contrast, brightness,\n    sharpness, identity\n]","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:51.870571Z","iopub.execute_input":"2025-09-21T09:53:51.870884Z","iopub.status.idle":"2025-09-21T09:53:51.899438Z","shell.execute_reply.started":"2025-09-21T09:53:51.870856Z","shell.execute_reply":"2025-09-21T09:53:51.898759Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# AdaAugment can be used to control augmentation magnitudes and operations\n\n\nclass AdaAugment:\n\n    def __init__(self, rand_m, rand_t):\n        self.key_transform = {}\n        self.key_magnitude = {}\n        self.transforms = [\n            shear_x, shear_y, \n            translate_x, translate_y,\n            rotate, equalize, \n            contrast, brightness,\n            sharpness, identity\n        ]\n        self.rand_m = rand_m\n        self.rand_t = rand_t\n\n    def set(self, keys, m=None, transform_idx=None):\n        if m is not None:\n            for i, key in enumerate(keys):\n                self.key_magnitude[key] = m[i].cpu().detach()\n            \n        if transform_idx is not None:\n            for i, key in enumerate(keys):\n                self.key_transform[key] = transform_idx[i].cpu().detach()\n    \n    def __call__(self, key, img):\n        \"\"\"\n        Apply an augmentation to the image based on stored magnitudes and transforms.\n        \n        Args:\n            key: unique identifier for the sample\n            img: input image to transform\n    \n        Returns:\n            Augmented image\n        \"\"\"\n        # --- Determine magnitude ---\n        magnitude = self.key_magnitude.get(key)\n        \n        if magnitude is None:\n            # No stored magnitude\n            if self.rand_m:\n                # Random magnitude\n                if random.random() < 0.6:\n                    return img  # skip transform 40% of the time\n                magnitude = random.random()\n            else:\n                magnitude = 0.0\n        else:\n            # Magnitude exists\n            if not self.rand_m and not self.rand_t:\n                # Use magnitude corresponding to stored transform index\n                transform_idx = int(self.key_transform.get(key))\n                if isinstance(magnitude, (list, torch.Tensor)):\n                    magnitude = float(magnitude[transform_idx])\n                else:\n                    magnitude = float(magnitude)\n            else:\n                magnitude = float(magnitude)\n    \n        # --- Determine transform ---\n        transform_idx = self.key_transform.get(key)\n        \n        if transform_idx is None:\n            # No stored transform\n            transform = random.choice(self.transforms) if self.rand_t else self.transforms[-1]\n        else:\n            transform = self.transforms[int(transform_idx)]\n    \n        # --- Apply transform ---\n        return transform(img, magnitude)\n\n\nclass SimpleAugment:\n\n    def __init__(self, p=0.6):\n        self.key_transform = {}\n        self.key_magnitude = {}\n        self.transforms = [\n            shear_x, shear_y, \n            translate_x, translate_y,\n            rotate, equalize, \n            contrast, brightness, \n            sharpness, identity\n        ]\n        self.transform_idx = list(range(len(self.transforms)))\n        self.p = p\n\n    def get(self, keys):\n        pass\n        \n    def __call__(self, key, img):\n        \"\"\"\n        Apply simple reandom augmentations to an image\n        \n        Args:\n            key: unique identifier for the sample\n            img: input image to transform\n    \n        Returns:\n            Augmented image\n        \"\"\"\n        if random.random() > self.p:\n            transform_idx = random.choice(self.transform_idx)\n            transform = self.transforms[transform_idx]\n            magnitude = random.random()\n            self.key_transform[key] = transform_idx\n            self.key_magnitude[key] = magnitude\n            return transform(img, magnitude)\n\n        else:\n            return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:53:51.900201Z","iopub.execute_input":"2025-09-21T09:53:51.900396Z","iopub.status.idle":"2025-09-21T09:53:51.920120Z","shell.execute_reply.started":"2025-09-21T09:53:51.900377Z","shell.execute_reply":"2025-09-21T09:53:51.919523Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Focal Loss for fine tuninng\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, weights:List=None, gamma=2.0, reduction=\"mean\"):\n        super().__init__()\n\n        self.weights = weights.to(device)\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logit, target):\n        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n            logit, target,\n            reduction=\"none\"\n        )\n        probs = torch.exp(-bce_loss)\n        F_loss = self.weights.to(target.device) * (1-probs) ** self.gamma * bce_loss\n\n        if self.reduction == \"mean\":\n            return F_loss.mean()\n        elif self.reduction == \"none\":\n            return F_loss   \n        else:\n            return F_loss.sum()","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:51.921822Z","iopub.execute_input":"2025-09-21T09:53:51.922430Z","iopub.status.idle":"2025-09-21T09:53:51.939080Z","shell.execute_reply.started":"2025-09-21T09:53:51.922412Z","shell.execute_reply":"2025-09-21T09:53:51.938513Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Saves per sample information such as previous loss etc.\n\nclass ValueMemory:\n    def __init__(self):\n        \"\"\"\n        Stores the last value per key (no EMA).\n        \"\"\"\n        self.values = {}\n\n    def __call__(self, keys, vals):\n        \"\"\"\n        keys: list of sample identifiers\n        vals: torch.Tensor of shape (len(keys), D)\n        Returns: current stored values, previous stored values\n        \"\"\"\n        stored_list = []\n        new_list = []\n\n        for i, key in enumerate(keys):\n            val = vals[i]\n            if key not in self.values:\n                old = val.clone()  # nothing stored yet → use current\n            else:\n                old = self.values[key]\n\n            self.values[key] = val  # overwrite with last value\n\n            stored_list.append(old.unsqueeze(0))\n            new_list.append(self.values[key].unsqueeze(0))\n\n        stored = torch.cat(stored_list, dim=0)  # previous values\n        return stored\n\n    def get(self, key):\n        \"\"\"\n        Access the stored value for a single key\n        \"\"\"\n        return self.values.get(key, None)\n\n    def get_multi(self, keys):\n        \"\"\"\n        Access stored values for multiple keys\n        \"\"\"\n        return torch.stack([self.values[k] for k in keys])","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:51.939893Z","iopub.execute_input":"2025-09-21T09:53:51.940318Z","iopub.status.idle":"2025-09-21T09:53:51.955784Z","shell.execute_reply.started":"2025-09-21T09:53:51.940301Z","shell.execute_reply":"2025-09-21T09:53:51.955155Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Setting up the AdaAugment Agent\n+ Critic => returns value for a state\n+ Actor => Parameterizes a beta distribution that is used to sample augmentation magnitudes\n+ Controller => Parameterizes a categorical distribution that is used to sample transformations from the augmentation space","metadata":{}},{"cell_type":"code","source":"class Actor(nn.Module):\n    def __init__(self, in_features, hidden, out_features):\n        super().__init__()\n        self.linear1 = nn.Linear(in_features, hidden)\n        self.layernorm1 = nn.LayerNorm(hidden)\n        self.linear2 = nn.Linear(hidden, hidden)\n        #self.layernorm2 = nn.LayerNorm(hidden)\n        self.alpha_head = nn.Linear(hidden, out_features)\n        self.beta_head = nn.Linear(hidden, out_features)\n\n    def forward(self, x):\n        x = torch.relu(self.layernorm1(self.linear1(x)))\n        x = torch.relu(self.linear2(x))\n        return torch.softmax(self.alpha_head(x), dim=-1) + 1, torch.softmax(self.beta_head(x), dim=-1) + 1\n\n    def get_dist(self, x):\n        alpha, beta = self(x)\n        dist = torch.distributions.Beta(alpha, beta)\n        return dist\n\n\nclass Critic(nn.Module):\n\n    def __init__(self, in_features, hidden):\n        super().__init__()\n        self.linear1 = nn.Linear(in_features, hidden)\n        self.layernorm1 = nn.LayerNorm(hidden)\n        self.linear2 = nn.Linear(hidden, hidden)\n        #self.layernorm2 = nn.LayerNorm(hidden)\n        self.head = nn.Linear(hidden, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layernorm1(self.linear1(x)))\n        x = torch.relu(self.linear2(x))\n        x = self.head(x)\n        return x\n\n\nclass Controller(nn.Module):\n\n    def __init__(self, in_features, hidden):\n        super().__init__()\n        self.linear1 = nn.Linear(in_features, hidden)\n        self.layernorm1 = nn.LayerNorm(hidden)\n        self.linear2 = nn.Linear(hidden, hidden)\n        #self.layernorm2 = nn.LayerNorm(hidden)\n        self.head = nn.Linear(hidden, len(augmentation_space))\n\n    def forward(self, x):\n        x = torch.relu(self.layernorm1(self.linear1(x)))\n        x = torch.relu(self.linear2(x))\n        x = self.head(x)\n        return x\n    \n    def get_dist(self, x):\n        out = self(x)\n        dist = torch.distributions.Categorical(logits=out)\n        return dist\n\n\nclass Agent(nn.Module):\n\n    def __init__(\n        self, \n        in_features,\n        out_features, \n        control=False, \n        actor=False, \n        advantage_reg=True,\n        observe=False\n    ):\n        super().__init__()\n        self.val_memory = ValueMemory()\n        self.loss_memory = ValueMemory()\n        self.advantage_reg = advantage_reg\n\n        self.critic = Critic(in_features=in_features, hidden=128)\n\n        self.store_ = {}\n\n        self.control_ = control\n        self.actor_ = actor\n        if control:\n            self.controller = Controller(in_features=in_features, hidden=128)\n            self.controller_optimizer = torch.optim.Adam(\n                params=self.controller.parameters(), lr=3e-5, weight_decay=5e-4\n            )\n\n        if actor:\n            self.actor = Actor(in_features=in_features, hidden=128, out_features=out_features) \n\n            self.actor_optimizer = torch.optim.Adam(\n                params=self.actor.parameters(), lr=3e-5, weight_decay=5e-4\n            )\n        self.critic_optimizer = torch.optim.Adam(\n            params=self.critic.parameters(), lr=3e-5, weight_decay=5e-4\n        )\n\n    def forward(self, state):\n        action_actor = None\n        action_controller = None\n\n        if self.actor_:  \n            dist = self.actor.get_dist(state.detach())\n            action_actor = dist.sample()\n            self.store_[\"action_actor\"] = action_actor\n            self.store_[\"dist_actor\"] = dist\n\n        if self.control_:\n            dist = self.controller.get_dist(state.detach())\n            action_controller = dist.sample()\n            self.store_[\"action_controller\"] = action_controller\n            self.store_[\"dist_controller\"] = dist\n\n        return action_actor, action_controller\n\n    def update(self, key, state, reward):\n        value = self.critic(state)\n        prev_state = self.val_memory(key, state)\n        prev_value = self.critic(prev_state)\n        \n        with torch.no_grad():\n            td_target = reward + 0.99 * value\n\n        if self.actor_ and self.control_:\n            dist_actor, action_actor = self.store_[\"dist_actor\"], self.store_[\"action_actor\"]\n            dist_control, action_control = self.store_[\"dist_controller\"], self.store_[\"action_controller\"]\n        \n            # Select magnitude corresponding to the chosen augmentation\n            chosen_magnitude = action_actor.gather(1, action_control.unsqueeze(1))\n\n            # Controller chooses augmentation index\n            aug_idx = action_control.unsqueeze(1)\n            \n            # Select alpha, beta for chosen augmentation\n            alpha_chosen = dist_actor.concentration1.gather(1, aug_idx)\n            beta_chosen  = dist_actor.concentration0.gather(1, aug_idx)\n            \n            # Build a Beta distribution only for the chosen augmentation\n            dist_chosen = torch.distributions.Beta(alpha_chosen.squeeze(1), beta_chosen.squeeze(1))\n            \n            # Compute log_prob of chosen magnitude\n            log_prob_actor = dist_chosen.log_prob(chosen_magnitude.squeeze(1))\n        \n            # Log-prob for augmentation choice\n            log_prob_control = dist_control.log_prob(action_control)\n        \n            # Total log prob\n            log_prob = log_prob_actor + log_prob_control\n\n        elif self.actor_:\n            dist, action = self.store_[\"dist_actor\"], self.store_[\"action_actor\"]\n            log_prob = dist.log_prob(action)\n\n        elif self.control_:\n            dist, action = self.store_[\"dist_controller\"], self.store_[\"action_controller\"]\n            log_prob = dist.log_prob(action)\n\n        advantage = td_target - prev_value.detach()\n\n        if self.advantage_reg:\n            advantage = (advantage - advantage.mean()) (advantage.std() + 1e-8)\n\n        actor_loss = -(log_prob * advantage).mean()\n\n        if self.control_:\n            self.controller_optimizer.zero_grad()\n        if self.actor_:\n            self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        if self.actor_:\n            self.actor_optimizer.step()\n        if self.control_:\n            self.controller_optimizer.step()\n        critic_loss = torch.nn.functional.mse_loss(td_target, prev_value)\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        return actor_loss, critic_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:53:51.956441Z","iopub.execute_input":"2025-09-21T09:53:51.956658Z","iopub.status.idle":"2025-09-21T09:53:51.976354Z","shell.execute_reply.started":"2025-09-21T09:53:51.956644Z","shell.execute_reply":"2025-09-21T09:53:51.975519Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Dataset\n+ Class weights\n+ Dataset class","metadata":{}},{"cell_type":"code","source":"def get_class_weights(df):\n    weights = []\n    label_columns = [\n        'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n        'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n        'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n    ]\n\n    for label in label_columns:\n        weight = len(df) / (df[label].sum() + 1e-6)\n        weights.append(weight)\n\n    return weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:53:51.977469Z","iopub.execute_input":"2025-09-21T09:53:51.977763Z","iopub.status.idle":"2025-09-21T09:53:51.993378Z","shell.execute_reply.started":"2025-09-21T09:53:51.977737Z","shell.execute_reply":"2025-09-21T09:53:51.992883Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class XRayDataset(Dataset):\n\n    def __init__(self, df, train=True, transform=None):\n        super().__init__()\n        self.df = df\n        self.train = train\n        self.transform = transform\n        self.label_columns = [\n            'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum',\n            'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion',\n            'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'\n        ]\n        if train:\n            class_weights = np.array(get_class_weights(df), dtype=np.float32)\n    \n            sample_weights = (df[label_columns].values.astype(np.float32) * class_weights).mean(axis=1)\n            sample_weights[sample_weights == 0] = 1.0\n            self.df[\"weight\"] = sample_weights\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx][\"img_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        label = self.df.iloc[idx][self.label_columns].values.astype(np.float32)\n        key = self.df.iloc[idx][\"Image_name\"]\n        \n        if self.train and self.transform is not None:\n            img = self.transform(key, img)\n\n        img = basic_transforms(img)\n\n        if not self.train:\n            return key, img\n\n        return key, img, torch.tensor(label, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:51.993998Z","iopub.execute_input":"2025-09-21T09:53:51.994228Z","iopub.status.idle":"2025-09-21T09:53:52.013459Z","shell.execute_reply.started":"2025-09-21T09:53:51.994213Z","shell.execute_reply":"2025-09-21T09:53:52.012830Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Adapt model to new dataset","metadata":{}},{"cell_type":"code","source":"train_csv = pd.read_csv(\"/kaggle/input/grand-xray-slam-division-a/train1.csv\")\ntrain_csv[\"img_path\"] = train_csv[\"Image_name\"].apply(lambda x: os.path.join(BASE_DIR, \"train1\", x))\nsubmission_df = pd.read_csv(\"/kaggle/input/grand-xray-slam-division-a/sample_submission_1.csv\")\ntrain_df, val_df = train_test_split(\n    train_csv,\n    test_size=0.2,\n    random_state=42,\n    stratify=train_csv[\"No Finding\"]\n)\n\nsubmission_df[\"img_path\"] = submission_df[\"Image_name\"].apply(lambda x : os.path.join(BASE_DIR, \"test1\", x))\nclass_weights = get_class_weights(train_df)\nzipped_class_weights = list(zip(label_columns, class_weights))\n\nprint(\"Train dataset:\")\nfor class_, weight in zipped_class_weights:\n    print(f\"{class_}: {weight:.2f}\\n\")\n\n\nprint(\"Validation dataset:\")\n\nclass_weights = get_class_weights(val_df)\nzipped_class_weights = list(zip(label_columns, class_weights))\nfor class_, weight in zipped_class_weights:\n    print(f\"{class_}: {weight:.2f}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:52.014183Z","iopub.execute_input":"2025-09-21T09:53:52.014420Z","iopub.status.idle":"2025-09-21T09:53:52.635969Z","shell.execute_reply.started":"2025-09-21T09:53:52.014406Z","shell.execute_reply":"2025-09-21T09:53:52.635215Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Atelectasis: 2.765\n\nCardiomegaly: 3.073\n\nConsolidation: 3.666\n\nEdema: 4.034\n\nEnlarged Cardiomediastinum: 2.841\n\nFracture: 7.221\n\nLung Lesion: 9.104\n\nLung Opacity: 2.211\n\nNo Finding: 3.162\n\nPleural Effusion: 3.137\n\nPleural Other: 15.225\n\nPneumonia: 7.571\n\nPneumothorax: 12.202\n\nSupport Devices: 2.852\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Dataset\nsimpleaugment = SimpleAugment(p=0.6)\n# get keys of samples that were augmented\n# get loss of those samples\n\n\n\ntrain_set = XRayDataset(train_df, train=True, transform=simpleaugment)\nval_set = XRayDataset(val_df, train=True)\ntest_set = XRayDataset(submission_df, train=False)\n\n# DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(\n    train_set, \n    batch_size=batch_size, \n    shuffle=True, \n    pin_memory=True,\n    num_workers=os.cpu_count()\n)\nval_loader = DataLoader(\n    val_set, \n    batch_size=batch_size, \n    pin_memory=True,\n    num_workers=os.cpu_count()\n)\ntest_loader = DataLoader(\n    test_set, \n    batch_size=batch_size, \n    pin_memory=True,\n    num_workers=os.cpu_count()\n)","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:52.636753Z","iopub.execute_input":"2025-09-21T09:53:52.637056Z","iopub.status.idle":"2025-09-21T09:53:52.672013Z","shell.execute_reply.started":"2025-09-21T09:53:52.637029Z","shell.execute_reply":"2025-09-21T09:53:52.671309Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Models and functions\nrandm, randt = True, True\nmodel_name = \"convnext\"\nget_model, in_features = models_[model_name]\nmodel = get_model(fine_tune=False)\nobserving_agent = Agent(in_features, len(augementation_space), not randt, not randm, observing=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = FocalLoss(weights=torch.tensor(class_weights).to(device))","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:52.672788Z","iopub.execute_input":"2025-09-21T09:53:52.672996Z","iopub.status.idle":"2025-09-21T09:53:54.183464Z","shell.execute_reply.started":"2025-09-21T09:53:52.672980Z","shell.execute_reply":"2025-09-21T09:53:54.182703Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n100%|██████████| 109M/109M [00:00<00:00, 201MB/s] \n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"epochs = 5\nmodel.to(device)\nbest_score = 0\nlr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=1e-3,\n    epochs=epochs,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.1\n)\n\n\nfor epoch in range(epochs):\n    \n    # Training\n    \n    model.train()\n    train_loss = 0\n    for keys, imgs, targets in tqdm(train_loader, desc=\"Training\"):\n        imgs, targets = imgs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n\n        logits = model(imgs)\n        loss = loss_fn(logits, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n\n    all_probs = []\n    all_targets = []\n\n    with torch.inference_mode():\n        for keys, imgs, targets in tqdm(val_loader, desc=\"Validation\"):\n            imgs, targets = imgs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n\n            logits = model(imgs)\n            loss = loss_fn(logits, targets)\n            probs = torch.sigmoid(logits)\n\n            val_loss += loss.item()\n\n            all_probs.append(probs)\n            all_targets.append(targets)\n\n    val_loss /= len(val_loader)\n    all_probs = torch.cat(all_probs, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n\n    all_preds = (all_probs >= 0.5).int()\n\n    val_accuracy = accuracy(all_preds, all_targets.int(), task=\"multilabel\", num_labels=14)\n    val_precision = precision(all_preds, all_targets.int(), task=\"multilabel\", num_labels=14)\n    val_recall = recall(all_preds, all_targets.int(), task=\"multilabel\", num_labels=14)\n    val_auroc = auroc(all_probs, all_targets.int(), task=\"multilabel\", num_labels=14)\n\n\n    if val_auroc > best_score:\n        best_score = val_auroc\n        initial_best_model = model.state_dict()\n        torch.save(initial_best_model, \"initial_best_model.pth\")\n        print(f\"New best auroc score: {best_score:.4f}\")\n\n    print(\n        f\"Epoch {epoch+1}/{epochs} | \"\n        f\"Train Loss: {train_loss:.4f} | \"\n        f\"Val Loss: {val_loss:.4f} | \"\n        f\"Acc: {val_accuracy:.4f} | \"\n        f\"Prec: {val_precision:.4f} | \"\n        f\"Rec: {val_recall:.4f} | \"\n        f\"AUROC: {val_auroc:.4f}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2025-09-21T09:53:54.184315Z","iopub.execute_input":"2025-09-21T09:53:54.184596Z","iopub.status.idle":"2025-09-21T12:18:19.851864Z","shell.execute_reply.started":"2025-09-21T09:53:54.184572Z","shell.execute_reply":"2025-09-21T12:18:19.851068Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [38:17<00:00,  1.71s/it]\nValidation: 100%|██████████| 336/336 [08:38<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.8748\nEpoch 1/3 | Train Loss: 0.4860 | Val Loss: 0.4489 | Acc: 0.8568 | Prec: 0.7364 | Rec: 0.6812 | AUROC: 0.8748\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [40:05<00:00,  1.79s/it]\nValidation: 100%|██████████| 336/336 [08:33<00:00,  1.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.8948\nEpoch 2/3 | Train Loss: 0.4189 | Val Loss: 0.4031 | Acc: 0.8704 | Prec: 0.7719 | Rec: 0.6966 | AUROC: 0.8948\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [40:14<00:00,  1.80s/it]\nValidation: 100%|██████████| 336/336 [08:35<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.8965\nEpoch 3/3 | Train Loss: 0.3940 | Val Loss: 0.4032 | Acc: 0.8735 | Prec: 0.8017 | Rec: 0.6680 | AUROC: 0.8965\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Finetuning using AdaAugment and Focal Loss","metadata":{}},{"cell_type":"markdown","source":"### Things to try out here:\n\n+ Add back the Layernorm after Linear1\n+ Advantage Norm\n+ Reward Norm\n+ Lr scheduler","metadata":{}},{"cell_type":"code","source":"randm, randt = False, False\n\nada_augment=AdaAugment(rand_m=randm, rand_t=randt)\ntrain_set = XRayDataset(train_df, train=True, transform=ada_augment)\n\nw_ = torch.tensor(train_set.df[\"weight\"], dtype=torch.float32)\nsampler = torch.utils.data.WeightedRandomSampler(\n    weights=w_,\n    num_samples=len(w_),\n    replacement=True\n)\n\n# New dataset using adaptive augmentations\ntrain_loader = DataLoader(\n    train_set, \n    batch_size=batch_size,\n    #sampler=sampler,\n    shuffle=True, \n    pin_memory=True,\n    num_workers=os.cpu_count()\n)\n\nget_model, in_features = models_[model_name]\n\nmodel = get_model(fine_tune=True)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/initial_best_model.pth\", map_location=device, weights_only=False))\n\noptimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n\nloss_fn = FocalLoss(weights=torch.tensor(class_weights, dtype=torch.float32), reduction=\"none\")\n\nagent = Agent(in_features, len(augmentation_space), not randt, not randm)\ncurrent_state = {}\nbest_score = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:18:19.904283Z","iopub.execute_input":"2025-09-21T12:18:19.904494Z","iopub.status.idle":"2025-09-21T12:18:21.038950Z","shell.execute_reply.started":"2025-09-21T12:18:19.904463Z","shell.execute_reply":"2025-09-21T12:18:21.038197Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def _register_head_hook(model):\n    def hook(module, input, output):\n        current_state['head_input'] = input[0].detach()\n    \n    if isinstance(model, torch.nn.DataParallel):\n        return model.module.classifier[-1].register_forward_hook(hook)\n    elif isinstance(model, models.ConvNeXt):\n        return model.classifier[2].register_forward_hook(hook)\n    else:\n        return model.classifier[-1].register_forward_hook(hook)\n\ndef _remove_head_hook(_hook_handle):\n    _hook_handle.remove()\n\n\nepochs = 5\nr_weight = 0.5\nr_norm = True\nmodel.to(device)\nagent.to(device)\n\n    \nfor epoch in range(epochs):\n    # Training\n    \n    model.train()\n    handle = _register_head_hook(model)\n    \n    train_loss, total_actor_loss, total_critic_loss = 0, 0, 0\n    for keys, imgs, targets in tqdm(train_loader, desc=\"Training\"):\n        imgs, targets = imgs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n\n        logits = model(imgs)\n        state = current_state[\"head_input\"]\n\n        loss = loss_fn(logits, targets)\n\n        prev_loss = agent.loss_memory(keys, loss.detach())\n        probs = torch.sigmoid(logits)\n\n        if randm or randt:\n    \n            action, transform_idx = agent(state)\n            ada_augment.set(keys, action, transform_idx)\n    \n            entropy = torch.sum(probs * torch.log(probs + 1e-8), dim=1).unsqueeze(1)\n            reward = r_weight * (loss.detach().mean() - prev_loss.mean()) + (1 - r_weight) * entropy\n            if r_norm:\n                reward = (reward - reward.mean()) / (reward.std() - 1e-8)\n    \n            actor_loss, critic_loss = agent.update(keys, state, reward)\n            total_actor_loss += actor_loss.item()\n            total_critic_loss += critic_loss.item()\n\n        loss = loss.mean()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    total_actor_loss /= len(train_loader)\n    total_critic_loss /= len(train_loader)\n\n    _remove_head_hook(handle)\n    # Validation\n    model.eval()\n    val_loss = 0\n\n    all_probs = []\n    all_targets = []\n\n    with torch.inference_mode():\n        for keys, imgs, targets in tqdm(val_loader, desc=\"Validation\"):\n            imgs, targets = imgs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n\n            logits = model(imgs)\n            loss = loss_fn(logits, targets)\n            loss = loss.mean()\n            probs = torch.sigmoid(logits)\n\n            val_loss += loss.item()\n\n            all_probs.append(probs)\n            all_targets.append(targets)\n\n    val_loss /= len(val_loader)\n    all_probs = torch.cat(all_probs, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n\n    all_preds = (all_probs >= 0.5).int()\n\n    val_accuracy = accuracy(all_preds, all_targets.int(), task=\"multilabel\", num_labels=14)\n    val_precision = precision(all_preds, all_targets.int(), task=\"multilabel\", num_labels=14)\n    val_recall = recall(all_preds, all_targets.int(), task=\"multilabel\", num_labels=14)\n    val_auroc = auroc(all_probs, all_targets.int(), task=\"multilabel\", num_labels=14)\n\n    if val_auroc > best_score:\n        best_score = val_auroc\n        initial_best_model = model.state_dict()\n        torch.save(initial_best_model, \"best_model.pth\")\n        print(f\"New best auroc score: {best_score:.4f}\")\n\n    print(\n        f\"Epoch {epoch+1}/{epochs} | \"\n        f\"Train Loss: {train_loss:.4f} | \"\n        f\"Actor Loss: {total_actor_loss:.4f} | \"\n        f\"Critic Loss: {total_critic_loss:.4f} | \"\n        f\"Val Loss: {val_loss:.4f} | \"\n        f\"Acc: {val_accuracy:.4f} | \"\n        f\"Prec: {val_precision:.4f} | \"\n        f\"Rec: {val_recall:.4f} | \"\n        f\"AUROC: {val_auroc:.4f}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T12:18:21.039831Z","iopub.execute_input":"2025-09-21T12:18:21.040433Z","iopub.status.idle":"2025-09-21T17:53:29.365512Z","shell.execute_reply.started":"2025-09-21T12:18:21.040399Z","shell.execute_reply":"2025-09-21T17:53:29.364556Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [33:32<00:00,  1.50s/it]\nValidation: 100%|██████████| 336/336 [08:34<00:00,  1.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.8996\nEpoch 1/5 | Train Loss: 0.3671 | Actor Loss: -4.2872 |Critic Loss: 3.7018 |Val Loss: 0.3924 | Acc: 0.8760 | Prec: 0.7881 | Rec: 0.7014 | AUROC: 0.8996\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [1:03:15<00:00,  2.83s/it]\nValidation: 100%|██████████| 336/336 [08:38<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.9005\nEpoch 2/5 | Train Loss: 0.4004 | Actor Loss: 0.3604 |Critic Loss: 65.4283 |Val Loss: 0.3906 | Acc: 0.8759 | Prec: 0.7811 | Rec: 0.7120 | AUROC: 0.9005\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [1:05:40<00:00,  2.93s/it]\nValidation: 100%|██████████| 336/336 [08:42<00:00,  1.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.9008\nEpoch 3/5 | Train Loss: 0.3967 | Actor Loss: -2.9447 |Critic Loss: 56.7383 |Val Loss: 0.3901 | Acc: 0.8762 | Prec: 0.7819 | Rec: 0.7118 | AUROC: 0.9008\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [1:04:18<00:00,  2.87s/it]\nValidation: 100%|██████████| 336/336 [09:01<00:00,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.9011\nEpoch 4/5 | Train Loss: 0.3955 | Actor Loss: -3.6595 |Critic Loss: 152.5614 |Val Loss: 0.3898 | Acc: 0.8763 | Prec: 0.7830 | Rec: 0.7110 | AUROC: 0.9011\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1343/1343 [1:04:28<00:00,  2.88s/it]\nValidation: 100%|██████████| 336/336 [08:53<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best auroc score: 0.9013\nEpoch 5/5 | Train Loss: 0.3936 | Actor Loss: -2.8835 |Critic Loss: 222.7858 |Val Loss: 0.3896 | Acc: 0.8763 | Prec: 0.7825 | Rec: 0.7116 | AUROC: 0.9013\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def create_submission(model_name=model_name):\n    get_model, _ = models_[model_name]\n    model = get_model()\n    model.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\", map_location=device, weights_only=False))\n\n    model.to(device)\n    model.eval()\n    \n    all_keys = []\n    all_probs = []\n    \n    with torch.inference_mode():\n        for key, img in tqdm(test_loader):\n            img = img.to(device)\n\n            out = model(img)\n            probs = torch.sigmoid(out)\n\n            all_keys.extend(key)\n            all_probs.append(probs.cpu().numpy())\n            \n    all_probs = np.concatenate(all_probs, axis=0)\n\n    return all_keys, all_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:17:17.286402Z","iopub.execute_input":"2025-09-21T18:17:17.286758Z","iopub.status.idle":"2025-09-21T18:17:17.292845Z","shell.execute_reply.started":"2025-09-21T18:17:17.286726Z","shell.execute_reply":"2025-09-21T18:17:17.292162Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"all_keys, all_probs = create_submission()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:17:18.855664Z","iopub.execute_input":"2025-09-21T18:17:18.856273Z","iopub.status.idle":"2025-09-21T18:34:47.482657Z","shell.execute_reply.started":"2025-09-21T18:17:18.856249Z","shell.execute_reply":"2025-09-21T18:34:47.481903Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 723/723 [17:26<00:00,  1.45s/it]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"threshold = 0.5\nbinary_ = False\nfinal_submission = pd.DataFrame()\nfinal_submission[\"Image_name\"] = all_keys\nif binary_:\n    final_submission[label_columns] = (probs_ > threshold).astype(int)\nelse:\n    final_submission[label_columns] = probs_\nfinal_submission.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"=\"* 70)\nprint(\"Final submission file created at /kaggle/working/submission.csv\")\nprint(\"=\"* 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:56:30.804934Z","iopub.execute_input":"2025-09-21T18:56:30.805602Z","iopub.status.idle":"2025-09-21T18:56:31.423776Z","shell.execute_reply.started":"2025-09-21T18:56:30.805575Z","shell.execute_reply":"2025-09-21T18:56:31.423016Z"}},"outputs":[{"name":"stdout","text":"============================================================\nFinal submission file created at /kaggle/working/submission.csv\n============================================================\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/working/submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T18:56:33.758674Z","iopub.execute_input":"2025-09-21T18:56:33.758942Z","iopub.status.idle":"2025-09-21T18:56:33.870308Z","shell.execute_reply.started":"2025-09-21T18:56:33.758923Z","shell.execute_reply":"2025-09-21T18:56:33.869599Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                 Image_name  Atelectasis  Cardiomegaly  Consolidation  \\\n0      00000005_001_001.jpg     0.220297      0.149363       0.195031   \n1      00000005_001_002.jpg     0.225101      0.214264       0.214019   \n2      00000005_002_001.jpg     0.720314      0.549541       0.610916   \n3      00000005_002_002.jpg     0.691454      0.401686       0.602385   \n4      00000007_001_001.jpg     0.586692      0.623392       0.428747   \n...                     ...          ...           ...            ...   \n46228  20009235_000_000.jpg     0.275094      0.163307       0.203308   \n46229  20009236_000_000.jpg     0.205679      0.128511       0.131184   \n46230  20009238_000_000.jpg     0.199245      0.130255       0.133074   \n46231  20009240_000_000.jpg     0.346170      0.223896       0.176529   \n46232  20009241_000_000.jpg     0.273891      0.206595       0.170680   \n\n          Edema  Enlarged Cardiomediastinum  Fracture  Lung Lesion  \\\n0      0.123469                    0.210709  0.305022     0.233154   \n1      0.186304                    0.278467  0.276405     0.255477   \n2      0.405752                    0.676551  0.461761     0.292920   \n3      0.266316                    0.526638  0.593861     0.363133   \n4      0.492691                    0.694068  0.263567     0.216696   \n...         ...                         ...       ...          ...   \n46228  0.161742                    0.098608  0.082475     0.346820   \n46229  0.098202                    0.087050  0.080408     0.279345   \n46230  0.102610                    0.093344  0.076683     0.273192   \n46231  0.144243                    0.143350  0.093923     0.204498   \n46232  0.164306                    0.124686  0.086882     0.266800   \n\n       Lung Opacity  No Finding  Pleural Effusion  Pleural Other  Pneumonia  \\\n0          0.258701    0.529039          0.193854       0.235400   0.223850   \n1          0.275641    0.379550          0.217344       0.197508   0.234026   \n2          0.745153    0.107114          0.633497       0.239078   0.392544   \n3          0.690712    0.113465          0.611649       0.352090   0.489921   \n4          0.654815    0.174051          0.480549       0.145823   0.236672   \n...             ...         ...               ...            ...        ...   \n46228      0.349803    0.513322          0.266759       0.206782   0.154588   \n46229      0.292245    0.639288          0.140768       0.156294   0.128068   \n46230      0.301521    0.619003          0.157510       0.168086   0.113717   \n46231      0.346055    0.581372          0.239030       0.141905   0.142443   \n46232      0.337728    0.552777          0.243443       0.190540   0.139593   \n\n       Pneumothorax  Support Devices  \n0          0.256924         0.454828  \n1          0.256407         0.632703  \n2          0.475623         0.698820  \n3          0.658006         0.696855  \n4          0.230011         0.654003  \n...             ...              ...  \n46228      0.200309         0.085841  \n46229      0.126906         0.079985  \n46230      0.146211         0.091032  \n46231      0.108583         0.116798  \n46232      0.133007         0.093387  \n\n[46233 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_name</th>\n      <th>Atelectasis</th>\n      <th>Cardiomegaly</th>\n      <th>Consolidation</th>\n      <th>Edema</th>\n      <th>Enlarged Cardiomediastinum</th>\n      <th>Fracture</th>\n      <th>Lung Lesion</th>\n      <th>Lung Opacity</th>\n      <th>No Finding</th>\n      <th>Pleural Effusion</th>\n      <th>Pleural Other</th>\n      <th>Pneumonia</th>\n      <th>Pneumothorax</th>\n      <th>Support Devices</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000005_001_001.jpg</td>\n      <td>0.220297</td>\n      <td>0.149363</td>\n      <td>0.195031</td>\n      <td>0.123469</td>\n      <td>0.210709</td>\n      <td>0.305022</td>\n      <td>0.233154</td>\n      <td>0.258701</td>\n      <td>0.529039</td>\n      <td>0.193854</td>\n      <td>0.235400</td>\n      <td>0.223850</td>\n      <td>0.256924</td>\n      <td>0.454828</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000005_001_002.jpg</td>\n      <td>0.225101</td>\n      <td>0.214264</td>\n      <td>0.214019</td>\n      <td>0.186304</td>\n      <td>0.278467</td>\n      <td>0.276405</td>\n      <td>0.255477</td>\n      <td>0.275641</td>\n      <td>0.379550</td>\n      <td>0.217344</td>\n      <td>0.197508</td>\n      <td>0.234026</td>\n      <td>0.256407</td>\n      <td>0.632703</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00000005_002_001.jpg</td>\n      <td>0.720314</td>\n      <td>0.549541</td>\n      <td>0.610916</td>\n      <td>0.405752</td>\n      <td>0.676551</td>\n      <td>0.461761</td>\n      <td>0.292920</td>\n      <td>0.745153</td>\n      <td>0.107114</td>\n      <td>0.633497</td>\n      <td>0.239078</td>\n      <td>0.392544</td>\n      <td>0.475623</td>\n      <td>0.698820</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00000005_002_002.jpg</td>\n      <td>0.691454</td>\n      <td>0.401686</td>\n      <td>0.602385</td>\n      <td>0.266316</td>\n      <td>0.526638</td>\n      <td>0.593861</td>\n      <td>0.363133</td>\n      <td>0.690712</td>\n      <td>0.113465</td>\n      <td>0.611649</td>\n      <td>0.352090</td>\n      <td>0.489921</td>\n      <td>0.658006</td>\n      <td>0.696855</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00000007_001_001.jpg</td>\n      <td>0.586692</td>\n      <td>0.623392</td>\n      <td>0.428747</td>\n      <td>0.492691</td>\n      <td>0.694068</td>\n      <td>0.263567</td>\n      <td>0.216696</td>\n      <td>0.654815</td>\n      <td>0.174051</td>\n      <td>0.480549</td>\n      <td>0.145823</td>\n      <td>0.236672</td>\n      <td>0.230011</td>\n      <td>0.654003</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>46228</th>\n      <td>20009235_000_000.jpg</td>\n      <td>0.275094</td>\n      <td>0.163307</td>\n      <td>0.203308</td>\n      <td>0.161742</td>\n      <td>0.098608</td>\n      <td>0.082475</td>\n      <td>0.346820</td>\n      <td>0.349803</td>\n      <td>0.513322</td>\n      <td>0.266759</td>\n      <td>0.206782</td>\n      <td>0.154588</td>\n      <td>0.200309</td>\n      <td>0.085841</td>\n    </tr>\n    <tr>\n      <th>46229</th>\n      <td>20009236_000_000.jpg</td>\n      <td>0.205679</td>\n      <td>0.128511</td>\n      <td>0.131184</td>\n      <td>0.098202</td>\n      <td>0.087050</td>\n      <td>0.080408</td>\n      <td>0.279345</td>\n      <td>0.292245</td>\n      <td>0.639288</td>\n      <td>0.140768</td>\n      <td>0.156294</td>\n      <td>0.128068</td>\n      <td>0.126906</td>\n      <td>0.079985</td>\n    </tr>\n    <tr>\n      <th>46230</th>\n      <td>20009238_000_000.jpg</td>\n      <td>0.199245</td>\n      <td>0.130255</td>\n      <td>0.133074</td>\n      <td>0.102610</td>\n      <td>0.093344</td>\n      <td>0.076683</td>\n      <td>0.273192</td>\n      <td>0.301521</td>\n      <td>0.619003</td>\n      <td>0.157510</td>\n      <td>0.168086</td>\n      <td>0.113717</td>\n      <td>0.146211</td>\n      <td>0.091032</td>\n    </tr>\n    <tr>\n      <th>46231</th>\n      <td>20009240_000_000.jpg</td>\n      <td>0.346170</td>\n      <td>0.223896</td>\n      <td>0.176529</td>\n      <td>0.144243</td>\n      <td>0.143350</td>\n      <td>0.093923</td>\n      <td>0.204498</td>\n      <td>0.346055</td>\n      <td>0.581372</td>\n      <td>0.239030</td>\n      <td>0.141905</td>\n      <td>0.142443</td>\n      <td>0.108583</td>\n      <td>0.116798</td>\n    </tr>\n    <tr>\n      <th>46232</th>\n      <td>20009241_000_000.jpg</td>\n      <td>0.273891</td>\n      <td>0.206595</td>\n      <td>0.170680</td>\n      <td>0.164306</td>\n      <td>0.124686</td>\n      <td>0.086882</td>\n      <td>0.266800</td>\n      <td>0.337728</td>\n      <td>0.552777</td>\n      <td>0.243443</td>\n      <td>0.190540</td>\n      <td>0.139593</td>\n      <td>0.133007</td>\n      <td>0.093387</td>\n    </tr>\n  </tbody>\n</table>\n<p>46233 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}